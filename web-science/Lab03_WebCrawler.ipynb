{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EaqjihtwRrys"
   },
   "source": [
    "# Lab03: Web Crawler (Continue) & Information Retrieval.\n",
    "\n",
    "- MSSV: 1712746\n",
    "- Họ và tên: Nguyễn Minh Tâm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUG7FFTsRryt"
   },
   "source": [
    "## Yêu cầu bài tập\n",
    "\n",
    "**Cách làm bài**\n",
    "\n",
    "\n",
    "Bạn sẽ làm trực tiếp trên file notebook này; từ `TODO` cho biết những phần mà bạn cần phải làm.\n",
    "\n",
    "Bạn có thể thảo luận ý tưởng cũng như tham khảo các tài liệu, nhưng *code và bài làm phải là của bạn*. \n",
    "\n",
    "Nếu vi phạm thì sẽ bị 0 điểm cho bài tập này.\n",
    "\n",
    "**Cách nộp bài**\n",
    "\n",
    "Trước khi nộp bài, rerun lại notebook (`Kernel` -> `Restart & Run All`).\n",
    "\n",
    "Sau đó, tạo thư mục có tên `MSSV` của bạn (vd, nếu bạn có MSSV là 1234567 thì bạn đặt tên thư mục là `1234567`) Chép file notebook, file `t_data.txt` và file `raw_data` của các bạn (nếu file này kích thước lớn các bạn có thể chép link vào `link_data.txt`), nén thư mục `MSSV` này lại và nộp trên moodle.\n",
    "\n",
    "**Nội dung bài tập**\n",
    "\n",
    "Cài đặt một web crawler để thu thập dữ liệu từ: https://en.wikipedia.org/wiki/Web_mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HlhdT6BRryu"
   },
   "source": [
    "## Nội dung bài tập"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9-ZyiLjRryv"
   },
   "source": [
    "Cài đặt một Web crawler đơn giản bắt đầu từ URL: https://en.wikipedia.org/wiki/Web_mining, tìm liên kết và thu thập dữ liệu trong HTML tại URL này sau đó lặp lại với các URL vừa tìm được.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "FJktAwbCOyod"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import string\n",
    "import pickle\n",
    "# addition\n",
    "import numpy as np\n",
    "import sys\n",
    "def drawProgressBar(percent, barLen = 20): # show percent complete https://stackoverflow.com/questions/3002085/python-to-print-out-status-bar-and-percentage\n",
    "    sys.stdout.write(\"\\r\")\n",
    "    progress = \"\"\n",
    "    for i in range(barLen):\n",
    "        if i < int(barLen * percent):\n",
    "            progress += \"=\"\n",
    "        else:\n",
    "            progress += \" \"\n",
    "    sys.stdout.write(\"[ %s ] %.2f%%\" % (progress, percent * 100))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mMSlOpSsRryv"
   },
   "source": [
    "## 1. Thu thập đường dẫn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZZ9lSUPRryw"
   },
   "source": [
    "- Robot.txt:  https://en.wikipedia.org/robots.txt\n",
    "- **Bước 1**: Thu thập đường dẫn từ https://en.wikipedia.org/wiki/Web_mining. Lưu trữ vào một danh sách `url_list`. \n",
    "- **Bước 2**: Lặp lại bước 1 cho các đường dẫn trong `url_list` (**lưu ý:** kiểm tra các đường dẫn vừa thu được đã nằm trong `url_list` hay không?). Dừng khi đã thu thập được 200 URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4HFWqw1VOrEe"
   },
   "outputs": [],
   "source": [
    "def valid_url(url): \n",
    "    # django url validation regex https://stackoverflow.com/questions/7160737/how-to-validate-a-url-in-python-malformed-or-not\n",
    "    regex = re.compile(\n",
    "        r'^(?:http|ftp)s?://' # http:// or https://\n",
    "        r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|' #domain...\n",
    "        r'localhost|' #localhost...\n",
    "        r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})' # ...or ip\n",
    "        r'(?::\\d+)?' # optional port\n",
    "        r'(?:/?|[/?]\\S+)$', re.IGNORECASE)\n",
    "    return (re.match(regex, url) is not None)\n",
    "\n",
    "\n",
    "def get_urls(url):\n",
    "    success = False\n",
    "    html_context = \"\"\n",
    "    try:\n",
    "        r=requests.get(url)\n",
    "        success = True\n",
    "        if(r.status_code != 200 or (\"text/html\" not in r.headers[\"content-type\"])):\n",
    "           return []\n",
    "        html_context = r.content\n",
    "    except:\n",
    "        success = False\n",
    "        return []\n",
    "    # TODO\n",
    "    # Lấy các url nằm trong trang web của url này, lưu lại vào biến urls\n",
    "    urls = []\n",
    "    soup = BeautifulSoup(html_context, \"lxml\") # lxml is just the parser for reading the html\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        if(valid_url(a['href'])): # check url, vì có khi là chỉ là url header hay sub url hay url image/pdf\n",
    "            urls.append(a['href'])\n",
    "    return urls\n",
    "\n",
    "def get_urls_recursive(start_url, limit):\n",
    "    urls = [start_url]\n",
    "    counter = 0\n",
    "    visited = {} # dict {url: True,...}\n",
    "    for url in urls:\n",
    "        # TODO\n",
    "        # Lấy các url nằm trong trang web của url này, lưu lại vào biến new_urls\n",
    "        # Với mỗi url mới trong new_urls:\n",
    "        #   Nếu nó chưa nằm trong urls thì thêm nó vô  \n",
    "        # Nếu kích thước của urls vượt quá limit thì dừng và xóa phần dư thừa\n",
    "        new_urls = get_urls(url)\n",
    "        for new_url in new_urls:\n",
    "            if(not hasattr(visited, new_url)):\n",
    "                visited[new_url] = True\n",
    "                urls.append(new_url)\n",
    "                counter += 1\n",
    "                if(counter >= limit):\n",
    "                    return urls\n",
    "        \n",
    "    return urls\n",
    "url_list = get_urls_recursive('https://en.wikipedia.org/wiki/Web_mining', 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zk4ty3jcRryx"
   },
   "source": [
    "## 2. Thu thập dữ liệu\n",
    "Thu thập dữ liệu từ `url_list`. Lưu trữ dữ liệu thu được vào dictionary data với keys là các từ, values gồm 2 phần tử: \n",
    "- `url_idx_list` với $idx \\in \\left[0,200\\right) \\cap \\mathbb{N}$\n",
    "- `frequency` \n",
    "    \n",
    "Ví dụ: `data['at']=[url_idx_list,frequency]`:\n",
    "- `url_idx_list`: danh sách các url mà trong dữ liệu của chúng (html document) chứa từ \"at\". \n",
    "- `frequency`: tần suất xuất hiện (số lần xuất hiện) của từ `at` trong dữ liệu của **tất cả đường dẫn thu được**.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YB5nIZAhQr7-"
   },
   "outputs": [],
   "source": [
    "def text_filter(element):\n",
    "    if element.parent.name in ['style', 'title', 'script', 'head', '[document]', 'class', 'a', 'li']:\n",
    "        return False\n",
    "    elif isinstance(element, Comment):\n",
    "        '''Opinion mining?'''\n",
    "        return False\n",
    "    elif re.match(r\"[\\s\\r\\n]+\",str(element)): \n",
    "        '''space, return, endline'''\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def wordList(url):\n",
    "    success = False\n",
    "    html_context = \"\"\n",
    "    try:\n",
    "        r=requests.get(url)\n",
    "        success = True\n",
    "        if(r.status_code != 200 or (\"text/html\" not in r.headers[\"content-type\"])):\n",
    "           return []\n",
    "        html_context = r.content\n",
    "    except:\n",
    "        success = False\n",
    "        return []\n",
    "        \n",
    "    soup=BeautifulSoup(html_context,\"html.parser\")\n",
    "    text=soup.findAll(text=True)\n",
    "    filtered_text=list(filter(text_filter,text))\n",
    "    word_list=[]\n",
    "    #TODO:\n",
    "    str_punc = string.punctuation\n",
    "    str_split = ''\n",
    "    for idx, c in enumerate(str_punc):\n",
    "        if(idx == 0):\n",
    "            str_split += f\"{c}\"\n",
    "        else:\n",
    "            str_split += f\"|{c}\"\n",
    "    \n",
    "    for sentence in filtered_text:\n",
    "        if(sentence not in str_punc):\n",
    "            next_words = re.sub('['+string.punctuation+']', '', sentence).split() #https://www.geeksforgeeks.org/python-extract-words-from-given-string/\n",
    "            word_list = word_list + next_words\n",
    "    return word_list\n",
    "\n",
    "def read_url(url, url_idx, data):\n",
    "    word_list=wordList(url)\n",
    "    \n",
    "    #TODO\n",
    "    '''Initialize value: data[w] = [[url_idx], 1]'''\n",
    "                                    #list_url\n",
    "    #VD: data.get(\"at\")==None => initialize data[\"at\"]\n",
    "        # data.get(\"at\")!=None >= add url_idx to list_url_idx (remember to check if this url_idx is in list_url_idx), \n",
    "                                # frequence+=1\n",
    "    for word in word_list:\n",
    "        if(data.get(word)==None):\n",
    "            data[word] = [[url_idx], 1]\n",
    "        else:\n",
    "            # update index urls\n",
    "            if url_idx not in data[word][0]:\n",
    "                data[word][0].append(url_idx)\n",
    "            # update count\n",
    "            data[word][1] += 1\n",
    "    return 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LrPuiiDhQfrJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ==================== ] 100.50%"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "for url_index, url in enumerate(url_list, 1):\n",
    "    read_url(url, url_index, data)\n",
    "    drawProgressBar((url_index+1) / len(url_list)) # just show complete percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "740eXy7pRryx"
   },
   "source": [
    "## 3. Tiền xử lý\n",
    "Loại bỏ các item trong data mà key là các stopword.\n",
    "\n",
    "**Ngữ liệu:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hC58K3Q3Rryy",
    "outputId": "99a20ea5-89c3-4d93-c74f-e43636b9b1cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mtosity/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "english_stopwords = stopwords.words('english')\n",
    "print(english_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cylo7trpRnun"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "for stopword in english_stopwords:\n",
    "    data.pop(stopword, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1   2   3   4  15  33  34  38  39  40  41  42  45  47  51  63  76  89\n",
      " 112 130 145 163 178]\n"
     ]
    }
   ],
   "source": [
    "w = np.array(data['web'][0])\n",
    "m = np.array(data['mining'][0])\n",
    "print(np.intersect1d(w, m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CEoxNVHqRry2"
   },
   "source": [
    "## 4. Lưu trữ và biểu diễn dữ liệu\n",
    "Sử dụng pickle lưu lại data với tên file raw_data.\n",
    "### 4.1 Cơ sở dữ liệu giao tác:\n",
    "Thông thường, các cơ sở dữ liệu giao tác được lưu trong flat files (các tập phẳng) thay vì trong một hệ cơ sở dữ liệu. Các item là các số nguyên không âm, mỗi giao tác tương ứng với một dòng các số nguyên phân tách nhau bằng khoảng trắng.\n",
    "Ví dụ:\n",
    "\n",
    "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 \n",
    "\n",
    "30 31 32 \n",
    "\n",
    "33 34 35 \n",
    "\n",
    "36 37 38 39 40 41 42 43 44 45 46 \n",
    "\n",
    "38 39 47 48 \n",
    "\n",
    "38 39 48 49 50 51 52 53 54 55 56 57 58 \n",
    "\n",
    "32 41 59 60 61 62 \n",
    "\n",
    "3 39 48 \n",
    "\n",
    "63 64 65 66 67 68 \n",
    "\n",
    "32 69 \n",
    "\n",
    "48 70 71 72 \n",
    "\n",
    "39 73 74 75 76 77 78 79 \n",
    "\n",
    "36 38 39 41 48 79 80 81 \n",
    "\n",
    "82 83 84 \n",
    "\n",
    "41 85 86 87 88 \n",
    "\n",
    "39 48 89 90 91 92 93 94 95 96 97 98 99 100 101 \n",
    "\n",
    "36 38 39 48 89 \n",
    "\n",
    "39 41 102 103 104 105 106 107 108 \n",
    "\n",
    "38 39 41 109 110 \n",
    "\n",
    "39 111 112 113 114 115 116 117 118 \n",
    "\n",
    "119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 \n",
    "\n",
    "48 134 135 136 \n",
    "\n",
    "39 48 137 138 139 140 141 142 143 144 145 146 147 148 149 \n",
    "\n",
    "39 150 151 152 \n",
    "\n",
    "38 39 56 153 154 155 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "4l--TVLEoN7R"
   },
   "outputs": [],
   "source": [
    "with open('raw_data', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvUgw0VkRry3"
   },
   "source": [
    "### 4.2 Xuất dataset\n",
    "Lưu một cơ sở dữ liệu giao tác (transactional database) vào file t_data.txt: \n",
    "- Các item tương ứng với url_idx\n",
    "- Mỗi transaction tương ứng với một từ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ZorK46LdSCfi"
   },
   "outputs": [],
   "source": [
    "with open('t_data.txt', 'w') as f:\n",
    "    for word, (url_list, freq) in data.items():\n",
    "        print(*url_list, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2OXVFoHiRry3"
   },
   "source": [
    "## 5. Truy vấn and, or, not\n",
    "Ví dụ: \n",
    "- Truy vấn `and` câu `web mining`: trả về đường dẫn tới các trang web có cả 2 từ web và từ mining. \n",
    "- Truy vấn `or` câu `web mining`: trả về đường dẫn tới các trang web có từ web hoặc từ mining.\n",
    "- Truy vấn `not` câu `web mining`: trả về đường dẫn tới các trang không có cả từ web và từ mining.\n",
    "\n",
    "*GỢI Ý: TÁCH CÂU TRUY VẤN THÀNH CÁC TỪ TƯƠNG TỰ PHƯƠNG PHÁP LÀM Ở LAB02.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "sybM0yLHRry6"
   },
   "outputs": [],
   "source": [
    "ret=[]\n",
    "def andRetrieval(ret, sentence):\n",
    "    '''Parameters\n",
    "    -----------------------\n",
    "    ret: url_list\n",
    "    sentence: query'''\n",
    "    # TODO\n",
    "    ### split sentence (separator ' ') into terms\n",
    "    ### find urls have all terms: urls\n",
    "    ### if len(ret)==0:  return urls\n",
    "    ### else update ret with urls: intersection of ret and urls \n",
    "    terms = sentence.split(' ')\n",
    "    urls = np.array([], dtype='uint')\n",
    "    for term_index, term in enumerate(terms):\n",
    "        if not None:\n",
    "            term_urls = np.array(data[term][0], dtype='uint')\n",
    "            if(term_index == 0):\n",
    "                urls = term_urls\n",
    "            else:\n",
    "                urls = np.intersect1d(urls, term_urls)# and\n",
    "    if(len(ret) == 0):\n",
    "        return urls\n",
    "    else:\n",
    "        ret = np.append(ret, urls)\n",
    "    return ret\n",
    "\n",
    "def orRetrieval(ret, sentence):\n",
    "    '''Parameters\n",
    "    -----------------------\n",
    "    ret: url_list\n",
    "    sentence: query'''\n",
    "    # TODO\n",
    "    ### split sentence (separator ' ') into terms\n",
    "    ### find urls have all terms: urls\n",
    "    ### find urls have at least 1 term: urls\n",
    "    ### update ret with urls: extend ret with urls\n",
    "    terms = sentence.split(' ')\n",
    "    urls = np.array([], dtype='uint')\n",
    "    for term_index, term in enumerate(terms):\n",
    "        if not None:\n",
    "            term_urls = np.array(data[term][0], dtype='uint')\n",
    "            urls = np.append(urls, term_urls) # or\n",
    "    if(len(ret) == 0):\n",
    "        return urls\n",
    "    else:\n",
    "        ret = np.append(ret, urls)\n",
    "    return ret\n",
    "\n",
    "def notRetrieval(ret, sentence):\n",
    "    '''Parameters\n",
    "    -----------------------\n",
    "    ret: url_list\n",
    "    sentence: query'''\n",
    "    # TODO\n",
    "    ### split sentence (separator ' ') into terms\n",
    "    ### find urls have at least 1 term: urls\n",
    "    ### update ret with urls: remove urls from ret \n",
    "    all_terms = data.keys()\n",
    "    not_terms = sentence.split(' ')\n",
    "    and_terms = [a for a in all_terms if a not in not_terms]\n",
    "    # basicly or\n",
    "    terms = and_terms\n",
    "    urls = np.array([], dtype='uint')\n",
    "    for term_index, term in enumerate(terms):\n",
    "        if not None:\n",
    "            term_urls = np.array(data[term][0], dtype='uint')\n",
    "            urls = np.append(urls, term_urls) # or\n",
    "    if(len(ret) == 0):\n",
    "        return urls\n",
    "    else:\n",
    "        ret = np.append(ret, urls)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "1hkp7ej3qxRQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1   2   3   4  15  33  34  38  39  40  41  42  45  47  51  63  76  89\n",
      " 112 130 145 163 178]\n",
      "[  1   2   3   4   7   8  15  21  33  34  35  38  39  40  41  42  45  47\n",
      "  48  51  53  56  63  64  66  69  76  77  79  82  89  90  92  95  98  99\n",
      " 103 107 109 110 111 112 113 115 117 119 120 130 131 132 136 140 142 143\n",
      " 144 145 146 148 150 152 153 163 164 165 169 173 175 176 177 178 179 181\n",
      " 183 185 186 196 198 199   1   2   3   4   5   6   9  10  15  16  17  23\n",
      "  33  34  36  38  39  40  41  42  45  47  49  50  51  52  62  63  65  75\n",
      "  76  78  88  89  91 112 130 145 163 178 197]\n",
      "[197.   1.   2. ... 197. 197. 197.]\n"
     ]
    }
   ],
   "source": [
    "print(andRetrieval([], 'web mining'))\n",
    "print(orRetrieval([], 'web mining'))\n",
    "print(notRetrieval(url_list, 'web mining'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Lab03-WebCrawler.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
