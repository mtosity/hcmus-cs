{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab02: Web Crawler (Continue).\n",
    "\n",
    "- MSSV: 1712746\n",
    "- Họ và tên: Nguyễn Minh Tâm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yêu cầu bài tập\n",
    "\n",
    "**Cách làm bài**\n",
    "\n",
    "\n",
    "Bạn sẽ làm trực tiếp trên file notebook này; trong file, từ `TODO` để cho biết những phần mà bạn cần phải làm.\n",
    "\n",
    "Bạn có thể thảo luận ý tưởng cũng như tham khảo các tài liệu, nhưng *code và bài làm phải là của bạn*. \n",
    "\n",
    "Nếu vi phạm thì sẽ bị 0 điểm cho bài tập này.\n",
    "\n",
    "**Cách nộp bài**\n",
    "\n",
    "Trước khi nộp bài, rerun lại notebook (`Kernel` -> `Restart & Run All`).\n",
    "\n",
    "Sau đó, tạo thư mục có tên `MSSV` của bạn (vd, nếu bạn có MSSV là 1234567 thì bạn đặt tên thư mục là `1234567`) Chép file notebook, file `output.txt` và file data các bạn đã lưu vào rồi nén thư mục `MSSV` này lại và nộp trên moodle.\n",
    "\n",
    "**Nội dung bài tập**\n",
    "\n",
    "Thu thập và thể hiện dữ liệu web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cài đặt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. HTML Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Bộ phân tích cú pháp HTML (HTML Parser): nhận HTML code và trích xuất thông tin liên quan như tiêu đề của trang, đoạn văn trong trang, tiêu đề trong trang, liên kết, văn bản in đậm, v.v."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start tag: html\n",
      "Start tag: head\n",
      "Start tag: title\n",
      "Data  : Test\n",
      "End tag : title\n",
      "End tag : head\n"
     ]
    }
   ],
   "source": [
    "from html.parser import HTMLParser\n",
    "\n",
    "class MyHTMLParser(HTMLParser):\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        print(\"Start tag:\", tag)\n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        print(\"End tag :\", tag)\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        print(\"Data  :\", data)\n",
    "\n",
    "parser = MyHTMLParser()\n",
    "parser.feed('<html><head><title>Test</title></head>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loại bỏ các HTML tag không cần thiết bằng cách thiết lập filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_filter(element):\n",
    "    if element.parent.name in ['style', 'title', 'script', 'head', '[document]', 'class', 'a', 'li']:\n",
    "        return False\n",
    "    elif isinstance(element, Comment):\n",
    "        '''Opinion mining?'''\n",
    "        return False\n",
    "    elif re.match(r\"[\\s\\r\\n]+\",str(element)): \n",
    "        '''space, return, endline'''\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Thu thập nội dung trang Web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trong bài tập này ta thể hiện tài liệu (hay nội dung trang web) với cấu trúc đơn giản như sau: \n",
    "- Gọi $D$ là một tập tài liệu chứa *n* tài liệu: $D=\\left\\{d_1,d_2,...,d_n\\right\\}$.\n",
    "- Ta thể hiện tài liệu bằng một dictionary `data={}` với `data[word]=[[url_idx_1,url_idx_2,...],frequency]` với `url_index`$\\in{\\left[{1,n}\\right]}$\n",
    "\n",
    "VD: `data['at']=[[1,2], 5]`\n",
    "Từ `at` xuất hiện trong đường dẫn có index `1` và `2` tổng số lần xuất hiện là 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bước 1: liệt kê các từ xuất hiện trong trang web:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordList(url):\n",
    "    r=requests.get(url)\n",
    "    soup=BeautifulSoup(r.content,\"html.parser\")\n",
    "    text=soup.findAll(text=True)\n",
    "    filtered_text=list(filter(text_filter,text))\n",
    "    word_list=[]\n",
    "    #TODO:\n",
    "    str_punc = string.punctuation\n",
    "    str_split = ''\n",
    "    for idx, c in enumerate(str_punc):\n",
    "        if(idx == 0):\n",
    "            str_split += f\"{c}\"\n",
    "        else:\n",
    "            str_split += f\"|{c}\"\n",
    "    \n",
    "    for sentence in filtered_text:\n",
    "        if(sentence not in str_punc):\n",
    "            next_words = re.sub('['+string.punctuation+']', '', sentence).split() #https://www.geeksforgeeks.org/python-extract-words-from-given-string/\n",
    "            word_list = word_list + next_words\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bước 2: Tính tần suất xuất hiện của từ trong 1 trang web, lưu trữ dữ liệu vào data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_url(url, url_idx, data):\n",
    "    word_list=wordList(url)\n",
    "    \n",
    "    #TODO\n",
    "    '''Initialize value: data[w] = [[url_idx], 1]'''\n",
    "                                    #list_url\n",
    "    #VD: data.get(\"at\")==None => initialize data[\"at\"]\n",
    "        # data.get(\"at\")!=None >= add url_idx to list_url_idx (remember to check if this url_idx is in list_url_idx), \n",
    "                                # frequence+=1\n",
    "    for word in word_list:\n",
    "        if(data.get(word)==None):\n",
    "            data[word] = [[url_idx], 1]\n",
    "        else:\n",
    "            # update index urls\n",
    "            if url_idx not in data[word][0]:\n",
    "                data[word][0].append(url_idx)\n",
    "            # update count\n",
    "            data[word][1] += 1\n",
    "    return 1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bước 3: Chạy chương trình lưu kết quả vào file output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "data={}\n",
    "read_url('https://en.wikipedia.org/wiki/Web_mining', 1,data)\n",
    "read_url('https://en.wikipedia.org/wiki/Data_mining', 2,data)\n",
    "\n",
    "sorted_keys = sorted(data.keys())\n",
    "\n",
    "with open(\"output.txt\", \"w\",encoding=\"utf-8\") as f:\n",
    "    output_line = \"Word\".ljust(20) + \"Frequency\".ljust(15) + \"URL_idx\".ljust(15) + \"\\n\"\n",
    "    f.writelines(output_line)\n",
    "    f.writelines('---------------------------------------------------------------------\\n\\n')\n",
    "    for key in sorted_keys:\n",
    "        output_string = str(key).ljust(20) + str(data[key][1]).ljust(15) + str(data[key][0]).ljust(15) + \"\\n\"\n",
    "        f.writelines(output_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Word                Frequency      URL_idx        \\n'\n",
      "b'---------------------------------------------------------------------\\n'\n",
      "b'\\n'\n",
      "b'1                   5              [1, 2]         \\n'\n",
      "b'10                  1              [2]            \\n'\n",
      "b'107                 1              [1]            \\n'\n",
      "b'11                  2              [2]            \\n'\n",
      "b'12                  2              [2]            \\n'\n",
      "b'13                  2              [2]            \\n'\n",
      "b'14                  3              [2]            \\n'\n",
      "b'14\\xe2\\x80\\x9331               1              [2]            \\n'\n",
      "b'15                  4              [1, 2]         \\n'\n"
     ]
    }
   ],
   "source": [
    "#TODO\n",
    "#Use python pickle save data. Load and print 10 first data values\n",
    "# 1712746: output.txt dep hon luc print ra :<\n",
    "num_lines = 10\n",
    "i = 0\n",
    "n_loop = num_lines + 2\n",
    "dumpfile_path = \"output.txt\"\n",
    "with open(dumpfile_path, \"rb\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        print(str(line))\n",
    "        i += 1\n",
    "        if(i >= n_loop):\n",
    "            break;\n",
    "       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
