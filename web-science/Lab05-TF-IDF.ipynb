{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QACY3c_lr4uz"
   },
   "source": [
    "# Lab05: TF-IDF.\n",
    "\n",
    "- MSSV: 1712746\n",
    "- Họ và tên: Nguyễn Minh Tâm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eD8WblEMr4u1"
   },
   "source": [
    "## Yêu cầu bài tập\n",
    "\n",
    "**Cách làm bài**\n",
    "\n",
    "\n",
    "Bạn sẽ làm trực tiếp trên file notebook này; từ `TODO` cho biết những phần mà bạn cần phải làm.\n",
    "\n",
    "Bạn có thể thảo luận ý tưởng cũng như tham khảo các tài liệu, nhưng *code và bài làm phải là của bạn*. \n",
    "\n",
    "Nếu vi phạm thì sẽ bị 0 điểm cho bài tập này.\n",
    "\n",
    "**Cách nộp bài**\n",
    "\n",
    "Trước khi nộp bài, rerun lại notebook (`Kernel` -> `Restart & Run All`).\n",
    "\n",
    "Sau đó, tạo thư mục có tên `MSSV` của bạn (vd, nếu bạn có MSSV là 1234567 thì bạn đặt tên thư mục là `1234567`) Chép file notebook, file `tf_idf_data.txt` của các bạn (nếu file này kích thước lớn các bạn có thể chép link vào `link_data.txt`), nén thư mục `MSSV` này lại và nộp trên moodle.\n",
    "\n",
    "**Nội dung bài tập**\n",
    "\n",
    "Cài đặt một web crawler để thu thập và biểu diễn dữ liệu bằng không gian vector và trọng số TF-IDF: https://en.wikipedia.org/wiki/Web_mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "txFiZE0Zr4u2"
   },
   "source": [
    "## Nội dung bài tập"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pU505-86r4u4"
   },
   "source": [
    "## 1. Không gian vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8DCPXdVrr4u4"
   },
   "source": [
    "- Một vector 2 chiều có thể được viết dưới dạng `[x,y]`\n",
    "- Một vector 3 chiều có thể được viết dưới dạng `[x,y,z]`\n",
    "![vector](vector.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Đặt $V$ là tập hợp các đặc trưng trong thể hiện dữ liệu.\n",
    "- Bất kỳ mẫu dữ liệu nào đều có thể được biểu diễn dưới dạng một vectơ với $\\vert V\\vert$ chiều\n",
    "- Ví dụ: giả sử chúng ta có 3 đặc trưng là các từ dog, bite, man. Giá trị 1 thể hiện từ đó xuất hiện 1 lần, 0 là không xuất hiện.  \n",
    "![space](space.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "TF-IDF cho biết độ quan trọng của một từ đối với một tài liệu trong ngữ liệu.\n",
    "\n",
    "TF- Term Frequency : dùng để ước lượng tần xuất xuất hiện của từ trong văn bản. Tuy nhiên với mỗi văn bản thì có độ dài khác nhau số lần xuất hiện của từ có thể nhiều hơn . Vì vậy số lần xuất hiện của từ sẽ được chuẩn hóa bằng cách chia cho độ dài của văn bản (tổng số từ trong văn bản đó).\n",
    "    \n",
    "$$tf_{t}=\\dfrac{f(t,d)}{\\sum_{i \\in d}f(i,d)}$$ \n",
    "\n",
    "- $f(t,d)$ là số lần xuất hiện từ $t$ trong văn bản $d$.\n",
    "\n",
    "IDF- Inverse Document Frequency: Khi tính tần số xuất hiện TF thì các từ đều được coi là quan trọng như nhau. Tuy nhiên có một số từ thường được được sử dụng nhiều nhưng không quan trọng để thể hiện ý nghĩa của đoạn văn như \"is\", \"the\"... (các từ chức năng). Ta cần giảm độ quan trọng của những từ này.\n",
    "\n",
    "$$idf_t=\\log \\left(\\dfrac{n}{df_t}\\right)$$\n",
    "\n",
    "- *n* là số văn bản.\n",
    "\n",
    "- $df_t$ là số văn bản xuất hiện từ t\n",
    "\n",
    "**TF-IDF:** $$tf_{t} \\times idf_t$$\n",
    "\n",
    "- $tf_{t}$ càng lớn tần suất xuất hiện của từ trong văn bản càng lớn.\n",
    "- $idf_t$ càng lớn từ hiếm khi xuất hiện trong tập dữ liệu.\n",
    "- **Giả định** những đặc trưng quan trọng nhất là những đặc trưng hiếm xuất hiện."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6kgzYNRKr4u5"
   },
   "source": [
    "## 3. Thu thập và thể hiện dữ liệu\n",
    "Thu thập dữ liệu bắt đầu từ URL: https://en.wikipedia.org/wiki/Web_mining.\n",
    "- Gọi $D$ là một tập văn bản chứa *n* văn bản: $D=\\left\\{d_1,d_2,...,d_n\\right\\}$.\n",
    "- $V=\\left\\{v_1,v_2,...v_{\\vert V \\vert}\\right\\}$ là từ điển (tất cả các từ phân biệt trong dữ liệu thu được). $\\vert V \\vert$ là kích thước từ điển.\n",
    "- Một trọng số $w_{ij}$ được gán cho từ $t_i$ của văn bản dj thuộc $D$ xác định mức quan trọng của $t_i$ trong văn bản $d_j$. Từ không xuất hiện trong $d_j$ có $w_{ij}=0$.\n",
    "- Mỗi văn bản $d_j$ được thể hiện dưới dạng vector $\\mathbf{d_j}= [w_{1j},w_{2j},...,w_{\\vert V \\vert j}]$\n",
    "- Thể hiện dữ liệu bằng một ma trận M kích thước $n \\times \\vert V \\vert$ => mỗi hàng thể hiện một văn bản."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mtosity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import string\n",
    "import pickle\n",
    "# addition\n",
    "import numpy as np\n",
    "import sys\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "\n",
    "def drawProgressBar(percent, barLen = 20): # show percent complete https://stackoverflow.com/questions/3002085/python-to-print-out-status-bar-and-percentage\n",
    "    sys.stdout.write(\"\\r\")\n",
    "    progress = \"\"\n",
    "    for i in range(barLen):\n",
    "        if i < int(barLen * percent):\n",
    "            progress += \"=\"\n",
    "        else:\n",
    "            progress += \" \"\n",
    "    sys.stdout.write(\"[ %s ] %.2f%%\" % (progress, percent * 100))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_url(url): \n",
    "    # django url validation regex https://stackoverflow.com/questions/7160737/how-to-validate-a-url-in-python-malformed-or-not\n",
    "    regex = re.compile(\n",
    "        r'^(?:http|ftp)s?://' # http:// or https://\n",
    "        r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|' #domain...\n",
    "        r'localhost|' #localhost...\n",
    "        r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})' # ...or ip\n",
    "        r'(?::\\d+)?' # optional port\n",
    "        r'(?:/?|[/?]\\S+)$', re.IGNORECASE)\n",
    "    return (re.match(regex, url) is not None)\n",
    "\n",
    "\n",
    "def get_urls(url):\n",
    "    success = False\n",
    "    html_context = \"\"\n",
    "    try:\n",
    "        r=requests.get(url)\n",
    "        success = True\n",
    "        if(r.status_code != 200 or (\"text/html\" not in r.headers[\"content-type\"])):\n",
    "           return []\n",
    "        html_context = r.content\n",
    "    except:\n",
    "        success = False\n",
    "        return []\n",
    "    # TODO\n",
    "    # Lấy các url nằm trong trang web của url này, lưu lại vào biến urls\n",
    "    urls = []\n",
    "    soup = BeautifulSoup(html_context, \"lxml\") # lxml is just the parser for reading the html\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        if(valid_url(a['href'])): # check url, vì có khi là chỉ là url header hay sub url hay url image/pdf\n",
    "            urls.append(a['href'])\n",
    "    return urls\n",
    "\n",
    "def get_urls_recursive(start_url, limit):\n",
    "    urls = [start_url]\n",
    "    counter = 1\n",
    "    visited = {} # dict {url: True,...}\n",
    "    for url in urls:\n",
    "        if(counter >= limit):\n",
    "            return urls\n",
    "        # TODO\n",
    "        # Lấy các url nằm trong trang web của url này, lưu lại vào biến new_urls\n",
    "        # Với mỗi url mới trong new_urls:\n",
    "        #   Nếu nó chưa nằm trong urls thì thêm nó vô  \n",
    "        # Nếu kích thước của urls vượt quá limit thì dừng và xóa phần dư thừa\n",
    "        new_urls = get_urls(url)\n",
    "        for new_url in new_urls:\n",
    "            if(counter >= limit):\n",
    "                return urls\n",
    "            if(not hasattr(visited, new_url)):\n",
    "                visited[new_url] = True\n",
    "                urls.append(new_url)\n",
    "                counter += 1\n",
    "        \n",
    "    return urls\n",
    "def text_filter(element):\n",
    "    if element.parent.name in ['style', 'title', 'script', 'head', '[document]', 'class', 'a', 'li']:\n",
    "        return False\n",
    "    elif isinstance(element, Comment):\n",
    "        '''Opinion mining?'''\n",
    "        return False\n",
    "    elif re.match(r\"[\\s\\r\\n]+\",str(element)): \n",
    "        '''space, return, endline'''\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def wordList(url):\n",
    "    success = False\n",
    "    html_context = \"\"\n",
    "    try:\n",
    "        r=requests.get(url)\n",
    "        success = True\n",
    "        if(r.status_code != 200 or (\"text/html\" not in r.headers[\"content-type\"])):\n",
    "           return []\n",
    "        html_context = r.content\n",
    "    except:\n",
    "        success = False\n",
    "        return []\n",
    "        \n",
    "    soup=BeautifulSoup(html_context,\"html.parser\")\n",
    "    text=soup.findAll(text=True)\n",
    "    filtered_text=list(filter(text_filter,text))\n",
    "    word_list=[]\n",
    "    #TODO:\n",
    "    str_punc = string.punctuation\n",
    "    str_split = ''\n",
    "    for idx, c in enumerate(str_punc):\n",
    "        if(idx == 0):\n",
    "            str_split += f\"{c}\"\n",
    "        else:\n",
    "            str_split += f\"|{c}\"\n",
    "    \n",
    "    for sentence in filtered_text:\n",
    "        if(sentence not in str_punc):\n",
    "            next_words = re.sub('['+string.punctuation+']', '', sentence).split() #https://www.geeksforgeeks.org/python-extract-words-from-given-string/\n",
    "            word_list = word_list + next_words\n",
    "    return word_list\n",
    "\n",
    "def read_url(url, url_idx, data):\n",
    "    word_list=wordList(url)\n",
    "    \n",
    "    #TODO\n",
    "    '''Initialize value: data[word] = [[url], 1]'''\n",
    "                                    #list_url\n",
    "    #VD: data.get(\"at\")==None => initialize data[\"at\"]\n",
    "        # data.get(\"at\")!=None >= add url_idx to list_url_idx (remember to check if this url_idx is in list_url_idx), \n",
    "                                # frequence+=1\n",
    "    for word in word_list:\n",
    "        if(data.get(word)==None):\n",
    "            data[word] = [[url_idx], 1]\n",
    "        else:\n",
    "            # update index urls\n",
    "            if url_idx not in data[word][0]:\n",
    "                data[word][0].append(url_idx)\n",
    "            # update count\n",
    "            data[word][1] += 1\n",
    "    return True\n",
    "\n",
    "def read_url_v2(url, url_idx, data):\n",
    "    word_list=wordList(url)\n",
    "    \n",
    "    #TODO\n",
    "    '''Initialize value: data[word] = [[url], [1]]'''\n",
    "                                    #list_url\n",
    "    #VD: data.get(\"at\")==None => initialize data[\"at\"]\n",
    "        # data.get(\"at\")!=None >= add url_idx to list_url_idx (remember to check if this url_idx is in list_url_idx), \n",
    "                                # frequence+=1\n",
    "    for word in word_list:\n",
    "        if(data.get(word)==None):\n",
    "            data[word] = [[url_idx], [1]]\n",
    "        else:\n",
    "            try:\n",
    "                data_word_url_index = data[word][0].index(url_idx)\n",
    "                data[word][1][data_word_url_index] += 1\n",
    "            except:\n",
    "                data[word][0].append(url_idx)\n",
    "                data[word][1].append(1)\n",
    "    return len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting urls\n",
      "Count words from urls\n",
      "[ ==================== ] 100.50%Removing stop words\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "n_urls=200\n",
    "print(\"Getting urls\");\n",
    "url_list = get_urls_recursive('https://en.wikipedia.org/wiki/Web_mining', n_urls)\n",
    "data = {} # {word: [[url_idx]: [count]]}\n",
    "num_words = [] # number of words for each url\n",
    "\n",
    "print(\"Count words from urls\");\n",
    "for url_index, url in enumerate(url_list, 1):\n",
    "    new_num_words = read_url_v2(url, url_index, data)\n",
    "    num_words.append(new_num_words)\n",
    "    drawProgressBar((url_index+1) / len(url_list)) # just show complete percentage\n",
    "    \n",
    "print(\"Removing stop words\")\n",
    "english_stopwords = stopwords.words('english')\n",
    "for stopword in english_stopwords:\n",
    "    data.pop(stopword, None)\n",
    "    \n",
    "n_words = len(data)\n",
    "list_words = list(data.keys())\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "M=np.zeros(shape=(n_urls,n_words),dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done converting data to M\n"
     ]
    }
   ],
   "source": [
    "for word_index, word in enumerate(list_words):\n",
    "    word_counting = data[word]\n",
    "    url_indexes = word_counting[0];\n",
    "    counts = word_counting[1];\n",
    "    for i in range(len(url_indexes)):\n",
    "        url_index = url_indexes[i];\n",
    "        count = counts[i];\n",
    "        M[url_index][word_index] += count\n",
    "        \n",
    "print('Done converting data to M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking shape:\n",
      "(200, 15189)\n",
      "200\n",
      "15189\n"
     ]
    }
   ],
   "source": [
    "print('checking shape:')\n",
    "print(M.shape)\n",
    "print(len(num_words)) # num_words[url_index] = count \n",
    "print(len(list_words)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF\n",
    "for i in range(len(num_words)):\n",
    "    if(num_words[i] != 0):\n",
    "        M[i] = M[i] / num_words[i]\n",
    "    \n",
    "# IDF\n",
    "for j in range(len(list_words)):\n",
    "    n = n_urls\n",
    "    word = list_words[j]\n",
    "    dft = len(data[word][0])\n",
    "    if(dft == 0):\n",
    "        print(word)\n",
    "    M[:, j] = M[:, j] * math.log(n / dft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.54018549e-02, 2.22881596e-02, 4.78912893e-04, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [2.79665260e-02, 3.42894764e-02, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       ...,\n",
       "       [5.26430959e-03, 1.23711577e-03, 0.00000000e+00, ...,\n",
       "        2.06803957e-03, 2.06803957e-03, 2.06803957e-03],\n",
       "       [1.18683220e-03, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 2.65605091e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tf_idf_data.txt\", \"wb\") as f:\n",
    "    np.save(f, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab03-WebCrawler.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
